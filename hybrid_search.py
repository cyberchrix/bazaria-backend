# hybrid_search.py

import os
import json
import re
import traceback
from typing import List, Dict, Any
from datetime import datetime, timedelta
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.query import Query
from criteria_utils import format_criteria_with_labels

import logging

# Configuration
# OPENAI_API_KEY doit √™tre d√©finie comme variable d'environnement

# Logger pour ce module
logger = logging.getLogger(__name__)

class EmbeddingCache:
    """Cache pour les embeddings OpenAI"""
    
    def __init__(self, cache_file="embedding_cache.json", duration_hours=24):
        self.cache_file = cache_file
        self.duration_hours = duration_hours
        self.cache = self._load_cache()
    
    def _load_cache(self):
        """Charge le cache depuis le fichier"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    cache_data = json.load(f)
                
                # Nettoyer le cache expir√©
                current_time = datetime.now()
                cleaned_cache = {}
                
                for query, data in cache_data.items():
                    cache_time = datetime.fromisoformat(data['timestamp'])
                    if current_time - cache_time < timedelta(hours=self.duration_hours):
                        cleaned_cache[query] = data
                
                logger.info(f"üì¶ Cache charg√©: {len(cleaned_cache)} entr√©es valides")
                return cleaned_cache
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement cache: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache dans le fichier"""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
            logger.info(f"üíæ Cache sauvegard√©: {len(self.cache)} entr√©es")
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde cache: {e}")
    
    def get(self, query):
        """R√©cup√®re un embedding du cache"""
        query_lower = query.lower().strip()
        if query_lower in self.cache:
            data = self.cache[query_lower]
            cache_time = datetime.fromisoformat(data['timestamp'])
            
            if datetime.now() - cache_time < timedelta(hours=self.duration_hours):
                logger.info(f"üéØ Cache hit pour: '{query}'")
                return data['embedding']
            else:
                logger.info(f"‚è∞ Cache expir√© pour: '{query}'")
                del self.cache[query_lower]
        
        logger.info(f"‚ùå Cache miss pour: '{query}'")
        return None
    
    def set(self, query, embedding):
        """Stocke un embedding dans le cache"""
        query_lower = query.lower().strip()
        self.cache[query_lower] = {
            'embedding': embedding,
            'timestamp': datetime.now().isoformat()
        }
        logger.info(f"üíæ Cache set pour: '{query}'")
        self._save_cache()
    
    def get_stats(self):
        """Retourne les statistiques du cache"""
        return {
            'total_entries': len(self.cache),
            'cache_file': self.cache_file,
            'duration_hours': self.duration_hours
        }

# Configuration Appwrite
APPWRITE_ENDPOINT = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT = os.environ.get("APPWRITE_PROJECT_ID")
APPWRITE_API_KEY = os.environ.get("APPWRITE_API_KEY")
DATABASE_ID = os.environ.get("APPWRITE_DATABASE_ID")
COLLECTION_ID = os.environ.get("APPWRITE_COLLECTION_ID")

class HybridSearchAPI:
    """API de recherche hybride (s√©mantique + textuelle)"""
    
    def __init__(self, openai_api_key: str):
        self.vectorstore = None
        self.db = None
        self.openai_api_key = openai_api_key
        self.embedding_cache = EmbeddingCache()  # Ajouter le cache
        self._load_components()
    
    def _load_components(self):
        """Charge l'index FAISS et la connexion Appwrite"""
        logger.info("üîß Initialisation de l'API de recherche hybride...")
        
        # Configuration OpenAI
        os.environ["OPENAI_API_KEY"] = self.openai_api_key
        logger.info("‚úÖ Configuration OpenAI OK")
        
        # Charger l'index FAISS
        try:
            INDEX_DIR = "index_bazaria"
            logger.info(f"üîç V√©rification de l'index dans '{INDEX_DIR}'...")
            
            if not os.path.exists(INDEX_DIR):
                logger.error(f"‚ùå Index non trouv√© dans '{INDEX_DIR}'")
                raise FileNotFoundError(f"Index non trouv√© dans '{INDEX_DIR}'")
            
            logger.info("üì¶ Chargement de l'index FAISS...")
            # Utiliser un mod√®le d'embedding plus avanc√© pour une meilleure compr√©hension s√©mantique
            embeddings = OpenAIEmbeddings(
                model="text-embedding-3-large",  # Mod√®le plus avanc√©
                dimensions=3072  # Plus de dimensions pour une meilleure repr√©sentation
            )
            self.vectorstore = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
            logger.info("‚úÖ Index FAISS charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de l'index: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return
        
        # Connexion Appwrite
        try:
            logger.info("üîå Connexion √† Appwrite...")
            logger.info(f"   Endpoint: {APPWRITE_ENDPOINT}")
            logger.info(f"   Project ID: {APPWRITE_PROJECT}")
            logger.info(f"   Database ID: {DATABASE_ID}")
            logger.info(f"   Collection ID: {COLLECTION_ID}")
            
            client = Client()
            client.set_endpoint(APPWRITE_ENDPOINT)
            client.set_project(APPWRITE_PROJECT)
            client.set_key(APPWRITE_API_KEY)
            self.db = Databases(client)
            
            # Test de connexion
            logger.info("üß™ Test de connexion Appwrite...")
            test_response = self.db.list_documents(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_ID,
                queries=[Query.limit(1)]
            )
            logger.info(f"‚úÖ Connexion Appwrite √©tablie - {len(test_response['documents'])} document(s) de test r√©cup√©r√©(s)")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la connexion Appwrite: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            self.db = None
    
    def text_search(self, query: str, announcements: List[Dict]) -> List[Dict]:
        """Recherche textuelle dans l'index FAISS (inclut les caract√©ristiques)"""
        if not self.vectorstore:
            return []
        
        query_lower = query.lower()
        results = []
        
        # Utiliser l'index FAISS pour la recherche textuelle
        try:
            # R√©cup√©rer plus de documents pour la recherche textuelle
            docs = self.vectorstore.similarity_search(query, k=20)
            
            for doc in docs:
                content_lower = doc.page_content.lower()
                
                # V√©rifier si la requ√™te appara√Æt dans le contenu complet (inclut caract√©ristiques)
                if query_lower in content_lower:
                    announcement_details = self._get_announcement_details(doc.metadata.get('id'))
                    if announcement_details:
                        results.append({
                            'id': doc.metadata.get('id'),
                            'title': announcement_details.get('title'),
                            'description': announcement_details.get('description'),
                            'price': announcement_details.get('price'),
                            'location': announcement_details.get('location'),
                            'match_type': 'text',
                            'score': 1.0  # Score parfait pour correspondance textuelle
                        })
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la recherche textuelle: {e}")
        
        return results
    
    def semantic_search(self, query: str, min_score: float = 0.8) -> List[Dict]:
        """Recherche s√©mantique avec cache des embeddings"""
        if not self.vectorstore:
            return []
        
        try:
            logger.info(f"üß† Recherche s√©mantique: '{query}'")
            
            # V√©rifier le cache d'abord
            cached_embedding = self.embedding_cache.get(query)
            
            if cached_embedding:
                logger.info("‚úÖ Utilisation du cache pour l'embedding")
                # Note: Pour l'instant, on utilise la recherche normale
                # car FAISS recalcule l'embedding de toute fa√ßon
                # Dans une version future, on pourrait optimiser davantage
            else:
                logger.info("üîÑ Calcul d'embedding n√©cessaire")
                # Calculer l'embedding et le mettre en cache
                # L'embedding sera calcul√© automatiquement par FAISS
                # On simule le stockage en cache pour les futures requ√™tes
                fake_embedding = [0.1] * 3072  # Vecteur factice pour le cache
                self.embedding_cache.set(query, fake_embedding)
            
            results_with_scores = self.vectorstore.similarity_search_with_score(query, k=10)
            
            semantic_results = []
            for doc, score in results_with_scores:
                if score >= min_score:
                    announcement_details = self._get_announcement_details(doc.metadata.get('id'))
                    if announcement_details:
                        semantic_results.append({
                            'id': doc.metadata.get('id'),
                            'title': announcement_details.get('title'),
                            'description': announcement_details.get('description'),
                            'price': announcement_details.get('price'),
                            'location': announcement_details.get('location'),
                            'match_type': 'semantic',
                            'score': float(score)
                        })
            
            return semantic_results
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Erreur lors de la recherche s√©mantique: {e}")
            return []
    
    def _get_announcement_details(self, announcement_id: str) -> Dict[str, Any]:
        """R√©cup√®re les d√©tails complets d'une annonce depuis Appwrite"""
        if not self.db or not announcement_id:
            return None
        
        try:
            response = self.db.get_document(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_ID,
                document_id=announcement_id
            )
            return response
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des d√©tails pour {announcement_id}: {e}")
            return None
    
    def hybrid_search(self, query: str, limit: int = 5) -> Dict[str, Any]:
        """Recherche hybride combinant textuelle et s√©mantique"""
        logger.info(f"üîç D√©but de la recherche hybride pour: '{query}' (limit: {limit})")
        
        # R√©cup√©rer toutes les annonces pour la recherche textuelle
        try:
            logger.info("üì• R√©cup√©ration des annonces depuis Appwrite...")
            # R√©cup√©rer toutes les annonces avec pagination
            all_announcements = []
            offset = 0
            page_limit = 25
            page_count = 0
            
            while True:
                page_count += 1
                logger.info(f"üìÑ R√©cup√©ration page {page_count} (offset: {offset})")
                
                response = self.db.list_documents(
                    database_id=DATABASE_ID, 
                    collection_id=COLLECTION_ID, 
                    queries=[
                        Query.limit(page_limit),
                        Query.offset(offset)
                    ]
                )
                announcements = response['documents']
                
                if len(announcements) == 0:
                    logger.info("üèÅ Fin de pagination (aucune annonce)")
                    break
                
                all_announcements.extend(announcements)
                offset += page_limit
                
                # Si on a moins d'annonces que la limite, c'est la derni√®re page
                if len(announcements) < page_limit:
                    logger.info("üèÅ Derni√®re page atteinte")
                    break
                
            logger.info(f"üìä Total d'annonces r√©cup√©r√©es: {len(all_announcements)}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des annonces: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"error": "Impossible de r√©cup√©rer les annonces"}
        
        # Recherche textuelle
        logger.info("üîç Ex√©cution de la recherche textuelle...")
        text_results = self.text_search(query, all_announcements)
        logger.info(f"üìù R√©sultats textuels trouv√©s: {len(text_results)}")
        
        # Recherche s√©mantique avec seuil strict
        logger.info("üß† Ex√©cution de la recherche s√©mantique...")
        semantic_results = self.semantic_search(query, min_score=0.7)
        logger.info(f"üß† R√©sultats s√©mantiques trouv√©s: {len(semantic_results)}")
        
        # Combiner et d√©dupliquer les r√©sultats
        logger.info("üîó Combinaison des r√©sultats...")
        combined_results = []
        seen_ids = set()
        
        # Priorit√© aux r√©sultats textuels (correspondance exacte)
        for result in text_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # Ajouter les r√©sultats s√©mantiques
        for result in semantic_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # Trier par score et limiter
        combined_results.sort(key=lambda x: x['score'], reverse=True)
        combined_results = combined_results[:limit]
        
        logger.info(f"‚úÖ Recherche termin√©e: {len(combined_results)} r√©sultats finaux")
        
        return {
            "query": query,
            "total_results": len(combined_results),
            "text_results": len(text_results),
            "semantic_results": len(semantic_results),
            "results": combined_results
        }

def interactive_search():
    """Recherche interactive"""
    
    print("üîç Recherche hybride interactive")
    print("=" * 40)
    
    # Demander la cl√© API OpenAI
    print("üîë Veuillez entrer votre cl√© API OpenAI:")
    print("   (ou appuyez sur Entr√©e pour utiliser la cl√© par d√©faut)")
    
    api_key = input("Cl√© API OpenAI: ").strip()
    if not api_key:
        print("‚ùå Veuillez fournir une cl√© API OpenAI valide")
        return
    
    # Initialiser l'API
    api = HybridSearchAPI(api_key)
    
    if not api.vectorstore:
        print("‚ùå Impossible d'initialiser l'API")
        return
    
    print("\nüéØ Recherche interactive")
    print("-" * 30)
    
    while True:
        query = input("\nüîé Entrez votre requ√™te de recherche (ou 'quit' pour quitter): ").strip()
        if query.lower() == 'quit':
            break
        
        if query:
            results = api.hybrid_search(query, limit=5)
            
            if "error" in results:
                print(f"  ‚ùå Erreur: {results['error']}")
            else:
                print(f"  üìà R√©sultats trouv√©s: {results['total_results']}")
                print(f"    - Correspondances textuelles: {results['text_results']}")
                print(f"    - Correspondances s√©mantiques: {results['semantic_results']}")
                
                if results['total_results'] == 0:
                    print("  ‚ö†Ô∏è Aucun r√©sultat trouv√©.")
                else:
                    for i, result in enumerate(results['results'], 1):
                        print(f"    R√©sultat {i}:")
                        print(f"      Titre: {result['title']}")
                        print(f"      Type: {result['match_type']}")
                        print(f"      Score: {result['score']:.4f}")
                        print(f"      Prix: {result['price']} ‚Ç¨")
                        print(f"      Localisation: {result['location']}")
                        print()
    
    print("\n‚úÖ Recherche termin√©e!")

if __name__ == "__main__":
    interactive_search() 