# hybrid_search.py

import os
import json
import re
import traceback
from typing import List, Dict, Any
from datetime import datetime, timedelta
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.query import Query
from criteria_utils import format_criteria_with_labels

import logging

# Configuration
# OPENAI_API_KEY doit √™tre d√©finie comme variable d'environnement

# Logger pour ce module
logger = logging.getLogger(__name__)

class EmbeddingCache:
    """Cache pour les embeddings OpenAI"""
    
    def __init__(self, cache_file="embedding_cache.json", duration_hours=24):
        # Utiliser le r√©pertoire persistant sur Render
        if os.path.exists("/opt/render/project/src/data"):
            self.cache_file = os.path.join("/opt/render/project/src/data", cache_file)
        elif os.path.exists("/var/data"):
            self.cache_file = os.path.join("/var/data", cache_file)
        else:
            self.cache_file = cache_file
        self.duration_hours = duration_hours
        self.cache = self._load_cache()
    
    def _load_cache(self):
        """Charge le cache depuis le fichier"""
        logger.info(f"üìÇ Tentative de chargement du cache depuis: {self.cache_file}")
        
        if os.path.exists(self.cache_file):
            try:
                logger.info(f"‚úÖ Fichier cache trouv√©: {self.cache_file}")
                with open(self.cache_file, 'r') as f:
                    cache_data = json.load(f)
                
                logger.info(f"üìñ Donn√©es lues: {len(cache_data)} entr√©es totales")
                
                # Nettoyer le cache expir√©
                current_time = datetime.now()
                cleaned_cache = {}
                
                for query, data in cache_data.items():
                    cache_time = datetime.fromisoformat(data['timestamp'])
                    if current_time - cache_time < timedelta(hours=self.duration_hours):
                        cleaned_cache[query] = data
                
                logger.info(f"üì¶ Cache charg√©: {len(cleaned_cache)} entr√©es valides")
                return cleaned_cache
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement cache: {e}")
                logger.error(f"üìÇ Fichier probl√©matique: {self.cache_file}")
                return {}
        else:
            logger.info(f"üìÇ Fichier cache non trouv√©: {self.cache_file}")
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache dans le fichier"""
        try:
            logger.info(f"üíæ Tentative de sauvegarde du cache vers: {self.cache_file}")
            
            # V√©rifier si le r√©pertoire existe
            import os
            cache_dir = os.path.dirname(self.cache_file)
            if cache_dir and not os.path.exists(cache_dir):
                logger.info(f"üìÅ Cr√©ation du r√©pertoire: {cache_dir}")
                os.makedirs(cache_dir, exist_ok=True)
            
            # V√©rifier les permissions
            logger.info(f"üîê V√©rification des permissions pour: {self.cache_file}")
            
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
            
            # V√©rifier que le fichier a √©t√© cr√©√©
            if os.path.exists(self.cache_file):
                file_size = os.path.getsize(self.cache_file)
                logger.info(f"‚úÖ Cache sauvegard√©: {len(self.cache)} entr√©es dans {self.cache_file} ({file_size} bytes)")
            else:
                logger.error(f"‚ùå Fichier non cr√©√©: {self.cache_file}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde cache: {e}")
            logger.error(f"üìÇ Fichier probl√©matique: {self.cache_file}")
            logger.error(f"üîç D√©tails de l'erreur: {str(e)}")
    
    def get(self, query):
        """R√©cup√®re un embedding du cache"""
        query_lower = query.lower().strip()
        logger.info(f"üîç Recherche dans le cache embedding: '{query}' (normalis√©: '{query_lower}')")
        
        if query_lower in self.cache:
            data = self.cache[query_lower]
            cache_time = datetime.fromisoformat(data['timestamp'])
            
            if datetime.now() - cache_time < timedelta(hours=self.duration_hours):
                logger.info(f"‚úÖ Cache hit pour: '{query}' (valide)")
                return data['embedding']
            else:
                logger.info(f"‚è∞ Cache expir√© pour: '{query}' (supprim√©)")
                del self.cache[query_lower]
        else:
            logger.info(f"‚ùå Cache miss pour: '{query}' (non trouv√©)")
        
        return None
    
    def set(self, query, embedding):
        """Stocke un embedding dans le cache"""
        query_lower = query.lower().strip()
        logger.info(f"üíæ Stockage dans le cache embedding: '{query}' (normalis√©: '{query_lower}')")
        
        self.cache[query_lower] = {
            'embedding': embedding,
            'timestamp': datetime.now().isoformat()
        }
        logger.info(f"‚úÖ Embedding mis en cache pour: '{query}'")
        self._save_cache()
    
    def get_stats(self):
        """Retourne les statistiques du cache"""
        return {
            'total_entries': len(self.cache),
            'cache_file': self.cache_file,
            'duration_hours': self.duration_hours
        }

class ResultCache:
    """Cache pour les r√©sultats de recherche complets avec TTL dynamique"""
    
    def __init__(self, cache_file="result_cache.json", duration_hours=2):  # Augment√© √† 2h
        # Utiliser le r√©pertoire persistant sur Render
        if os.path.exists("/opt/render/project/src/data"):
            self.cache_file = os.path.join("/opt/render/project/src/data", cache_file)
        elif os.path.exists("/var/data"):
            self.cache_file = os.path.join("/var/data", cache_file)
        else:
            self.cache_file = cache_file
        self.duration_hours = duration_hours
        self.cache = self._load_cache()
    
    def _load_cache(self):
        """Charge le cache depuis le fichier"""
        logger.info(f"üìÇ Tentative de chargement du cache r√©sultats depuis: {self.cache_file}")
        
        if os.path.exists(self.cache_file):
            try:
                logger.info(f"‚úÖ Fichier cache r√©sultats trouv√©: {self.cache_file}")
                with open(self.cache_file, 'r') as f:
                    cache_data = json.load(f)
                
                logger.info(f"üìñ Donn√©es r√©sultats lues: {len(cache_data)} entr√©es totales")
                
                # Nettoyer le cache expir√©
                current_time = datetime.now()
                cleaned_cache = {}
                
                for query, data in cache_data.items():
                    cache_time = datetime.fromisoformat(data['timestamp'])
                    if current_time - cache_time < timedelta(hours=self.duration_hours):
                        cleaned_cache[query] = data
                
                logger.info(f"üì¶ Cache r√©sultats charg√©: {len(cleaned_cache)} entr√©es valides")
                return cleaned_cache
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement cache r√©sultats: {e}")
                logger.error(f"üìÇ Fichier probl√©matique: {self.cache_file}")
                return {}
        else:
            logger.info(f"üìÇ Fichier cache r√©sultats non trouv√©: {self.cache_file}")
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache dans le fichier"""
        try:
            logger.info(f"üíæ Tentative de sauvegarde du cache r√©sultats vers: {self.cache_file}")
            
            # V√©rifier si le r√©pertoire existe
            import os
            cache_dir = os.path.dirname(self.cache_file)
            if cache_dir and not os.path.exists(cache_dir):
                logger.info(f"üìÅ Cr√©ation du r√©pertoire: {cache_dir}")
                os.makedirs(cache_dir, exist_ok=True)
            
            # V√©rifier les permissions
            logger.info(f"üîê V√©rification des permissions pour: {self.cache_file}")
            
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
            
            # V√©rifier que le fichier a √©t√© cr√©√©
            if os.path.exists(self.cache_file):
                file_size = os.path.getsize(self.cache_file)
                logger.info(f"‚úÖ Cache r√©sultats sauvegard√©: {len(self.cache)} entr√©es dans {self.cache_file} ({file_size} bytes)")
            else:
                logger.error(f"‚ùå Fichier non cr√©√©: {self.cache_file}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde cache r√©sultats: {e}")
            logger.error(f"üìÇ Fichier probl√©matique: {self.cache_file}")
            logger.error(f"üîç D√©tails de l'erreur: {str(e)}")
    
    def get(self, query):
        """R√©cup√®re un r√©sultat du cache"""
        query_lower = query.lower().strip()
        logger.info(f"üîç Recherche dans le cache r√©sultats: '{query}' (normalis√©: '{query_lower}')")
        
        if query_lower in self.cache:
            data = self.cache[query_lower]
            cache_time = datetime.fromisoformat(data['timestamp'])
            
            if datetime.now() - cache_time < timedelta(hours=self.duration_hours):
                logger.info(f"‚úÖ Cache hit r√©sultats pour: '{query}' (valide)")
                return data['results']
            else:
                logger.info(f"‚è∞ Cache r√©sultats expir√© pour: '{query}' (supprim√©)")
                del self.cache[query_lower]
        else:
            logger.info(f"‚ùå Cache miss r√©sultats pour: '{query}' (non trouv√©)")
        
        return None
    
    def set(self, query, results):
        """Stocke un r√©sultat dans le cache"""
        query_lower = query.lower().strip()
        logger.info(f"üíæ Stockage dans le cache r√©sultats: '{query}' (normalis√©: '{query_lower}')")
        
        self.cache[query_lower] = {
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
        logger.info(f"‚úÖ R√©sultats mis en cache pour: '{query}' ({len(results)} r√©sultats)")
        self._save_cache()
    
    def get_stats(self):
        """Retourne les statistiques du cache"""
        return {
            'total_entries': len(self.cache),
            'cache_file': self.cache_file,
            'duration_hours': self.duration_hours
        }

# Configuration Appwrite
APPWRITE_ENDPOINT = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT = os.environ.get("APPWRITE_PROJECT_ID")
APPWRITE_API_KEY = os.environ.get("APPWRITE_API_KEY")
DATABASE_ID = os.environ.get("APPWRITE_DATABASE_ID")
COLLECTION_ID = os.environ.get("APPWRITE_COLLECTION_ID")

class HybridSearchAPI:
    """API de recherche hybride (s√©mantique + textuelle)"""
    
    def __init__(self, openai_api_key: str):
        self.vectorstore = None
        self.db = None
        self.openai_api_key = openai_api_key
        self.embedding_cache = EmbeddingCache()  # Cache des embeddings
        self.result_cache = ResultCache()  # Cache des r√©sultats
        self._load_components()
    
    def _load_components(self):
        """Charge l'index FAISS et la connexion Appwrite"""
        logger.info("üîß Initialisation de l'API de recherche hybride...")
        
        # Configuration OpenAI
        os.environ["OPENAI_API_KEY"] = self.openai_api_key
        logger.info("‚úÖ Configuration OpenAI OK")
        
        # Charger l'index FAISS
        try:
            INDEX_DIR = "index_bazaria"
            logger.info(f"üîç V√©rification de l'index dans '{INDEX_DIR}'...")
            
            if not os.path.exists(INDEX_DIR):
                logger.error(f"‚ùå Index non trouv√© dans '{INDEX_DIR}'")
                raise FileNotFoundError(f"Index non trouv√© dans '{INDEX_DIR}'")
            
            logger.info("üì¶ Chargement de l'index FAISS...")
            # Utiliser un mod√®le d'embedding plus avanc√© pour une meilleure compr√©hension s√©mantique
            self.embeddings = OpenAIEmbeddings(
                model="text-embedding-3-large",  # Mod√®le plus avanc√©
                dimensions=3072  # Plus de dimensions pour une meilleure repr√©sentation
            )
            self.vectorstore = FAISS.load_local(INDEX_DIR, self.embeddings, allow_dangerous_deserialization=True)
            logger.info("‚úÖ Index FAISS charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de l'index: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return
        
        # Connexion Appwrite
        try:
            logger.info("üîå Connexion √† Appwrite...")
            logger.info(f"   Endpoint: {APPWRITE_ENDPOINT}")
            logger.info(f"   Project ID: {APPWRITE_PROJECT}")
            logger.info(f"   Database ID: {DATABASE_ID}")
            logger.info(f"   Collection ID: {COLLECTION_ID}")
            
            client = Client()
            client.set_endpoint(APPWRITE_ENDPOINT)
            client.set_project(APPWRITE_PROJECT)
            client.set_key(APPWRITE_API_KEY)
            self.db = Databases(client)
            
            # Test de connexion
            logger.info("üß™ Test de connexion Appwrite...")
            test_response = self.db.list_documents(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_ID,
                queries=[Query.limit(1)]
            )
            logger.info(f"‚úÖ Connexion Appwrite √©tablie - {len(test_response['documents'])} document(s) de test r√©cup√©r√©(s)")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la connexion Appwrite: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            self.db = None
    
    def text_search(self, query: str, announcements: List[Dict]) -> List[Dict]:
        """Recherche textuelle dans l'index FAISS (inclut les caract√©ristiques)"""
        if not self.vectorstore:
            return []
        
        query_lower = query.lower()
        results = []
        
        # Utiliser l'index FAISS pour la recherche textuelle
        try:
            # R√©cup√©rer plus de documents pour la recherche textuelle
            docs = self.vectorstore.similarity_search(query, k=20)
            
            for doc in docs:
                content_lower = doc.page_content.lower()
                
                # V√©rifier si la requ√™te appara√Æt dans le contenu complet (inclut caract√©ristiques)
                if query_lower in content_lower:
                    announcement_details = self._get_announcement_details(doc.metadata.get('id'))
                    if announcement_details:
                        results.append({
                            'id': doc.metadata.get('id'),
                            'title': announcement_details.get('title'),
                            'description': announcement_details.get('description'),
                            'price': announcement_details.get('price'),
                            'location': announcement_details.get('location'),
                            'match_type': 'text',
                            'score': 1.0  # Score parfait pour correspondance textuelle
                        })
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la recherche textuelle: {e}")
        
        return results
    
    def semantic_search(self, query: str, min_score: float = 0.8) -> List[Dict]:
        """Recherche s√©mantique avec cache optimis√© (embeddings + r√©sultats)"""
        if not self.vectorstore:
            return []
        
        try:
            logger.info(f"üß† Recherche s√©mantique: '{query}'")
            
            # 1. V√©rifier le cache des r√©sultats complets (le plus rapide)
            logger.info(f"üîç V√©rification du cache des r√©sultats pour: '{query}'")
            cached_results = self.result_cache.get(query)
            if cached_results:
                logger.info(f"‚úÖ Cache hit - r√©sultats complets trouv√©s pour: '{query}'")
                return cached_results
            else:
                logger.info(f"‚ùå Cache miss - r√©sultats complets non trouv√©s pour: '{query}'")
            
            # 2. V√©rifier le cache des embeddings
            logger.info(f"üîç V√©rification du cache des embeddings pour: '{query}'")
            cached_embedding = self.embedding_cache.get(query)
            
            if cached_embedding:
                logger.info(f"‚úÖ Cache hit - embedding trouv√© pour: '{query}'")
                # Utiliser l'embedding en cache pour la recherche FAISS
                results_with_scores = self.vectorstore.similarity_search_by_vector(
                    cached_embedding, k=20  # Plus de r√©sultats pour un meilleur tri
                )
                logger.info(f"üîç Recherche FAISS avec embedding en cache: {len(results_with_scores)} r√©sultats")
            else:
                logger.info(f"‚ùå Cache miss - embedding non trouv√© pour: '{query}'")
                logger.info(f"üîÑ Calcul d'embedding OpenAI pour: '{query}'")
                # Calculer l'embedding r√©el et le mettre en cache
                embedding = self.embeddings.embed_query(query)
                logger.info(f"‚úÖ Embedding calcul√© et mis en cache pour: '{query}'")
                self.embedding_cache.set(query, embedding)
                
                # Recherche avec l'embedding calcul√©
                results_with_scores = self.vectorstore.similarity_search_by_vector(
                    embedding, k=20  # Plus de r√©sultats pour un meilleur tri
                )
                logger.info(f"üîç Recherche FAISS avec nouvel embedding: {len(results_with_scores)} r√©sultats")
            
            # 3. Formater les r√©sultats
            logger.info(f"üìù Formatage des r√©sultats pour: '{query}'")
            semantic_results = []
            
            # Calculer les scores de similarit√© r√©els
            for i, doc in enumerate(results_with_scores):
                # Score bas√© sur la position dans les r√©sultats (plus proche = meilleur score)
                # Les premiers r√©sultats sont les plus pertinents
                base_score = 1.0 - (i * 0.1)  # D√©croissance lin√©aire
                score = max(base_score, min_score)  # Minimum d√©fini par min_score
                
                if score >= min_score:
                    announcement_details = self._get_announcement_details(doc.metadata.get('id'))
                    if announcement_details:
                        semantic_results.append({
                            'id': doc.metadata.get('id'),
                            'title': announcement_details.get('title'),
                            'description': announcement_details.get('description'),
                            'price': announcement_details.get('price'),
                            'location': announcement_details.get('location'),
                            'match_type': 'semantic',
                            'score': float(score)
                        })
            
            # Trier par score d√©croissant pour une meilleure pertinence
            semantic_results.sort(key=lambda x: x['score'], reverse=True)
            
            logger.info(f"‚úÖ {len(semantic_results)} r√©sultats format√©s pour: '{query}'")
            
            # 4. Mettre en cache les r√©sultats complets
            logger.info(f"üíæ Mise en cache des r√©sultats complets pour: '{query}'")
            self.result_cache.set(query, semantic_results)
            
            return semantic_results
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Erreur lors de la recherche s√©mantique: {e}")
            return []
    
    def semantic_search_advanced(self, query: str, min_score: float = 0.7, max_results: int = 15) -> List[Dict]:
        """Recherche s√©mantique avanc√©e avec param√®tres optimis√©s"""
        if not self.vectorstore:
            return []
        
        try:
            logger.info(f"üß† Recherche s√©mantique avanc√©e: '{query}' (min_score: {min_score}, max_results: {max_results})")
            
            # 1. V√©rifier le cache des r√©sultats complets
            cached_results = self.result_cache.get(query)
            if cached_results:
                logger.info(f"‚úÖ Cache hit - r√©sultats complets trouv√©s pour: '{query}'")
                # Filtrer et limiter les r√©sultats en cache
                filtered_results = [r for r in cached_results if r['score'] >= min_score][:max_results]
                return filtered_results
            
            # 2. V√©rifier le cache des embeddings
            cached_embedding = self.embedding_cache.get(query)
            
            if cached_embedding:
                logger.info(f"‚úÖ Cache hit - embedding trouv√© pour: '{query}'")
                results_with_scores = self.vectorstore.similarity_search_by_vector(
                    cached_embedding, k=max_results * 2  # Plus de r√©sultats pour un meilleur tri
                )
            else:
                logger.info(f"üîÑ Calcul d'embedding OpenAI pour: '{query}'")
                embedding = self.embeddings.embed_query(query)
                self.embedding_cache.set(query, embedding)
                
                results_with_scores = self.vectorstore.similarity_search_by_vector(
                    embedding, k=max_results * 2
                )
            
            # 3. Formater les r√©sultats avec scores am√©lior√©s
            semantic_results = []
            for i, doc in enumerate(results_with_scores):
                # Score bas√© sur la position et la similarit√©
                position_score = 1.0 - (i * 0.05)  # D√©croissance plus douce
                score = max(position_score, min_score)
                
                if score >= min_score:
                    announcement_details = self._get_announcement_details(doc.metadata.get('id'))
                    if announcement_details:
                        semantic_results.append({
                            'id': doc.metadata.get('id'),
                            'title': announcement_details.get('title'),
                            'description': announcement_details.get('description'),
                            'price': announcement_details.get('price'),
                            'location': announcement_details.get('location'),
                            'match_type': 'semantic',
                            'score': float(score)
                        })
            
            # Trier par score et limiter
            semantic_results.sort(key=lambda x: x['score'], reverse=True)
            semantic_results = semantic_results[:max_results]
            
            # Mettre en cache les r√©sultats complets
            self.result_cache.set(query, semantic_results)
            
            logger.info(f"‚úÖ {len(semantic_results)} r√©sultats avanc√©s pour: '{query}'")
            return semantic_results
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Erreur lors de la recherche s√©mantique avanc√©e: {e}")
            return []
    
    def search_with_price_filter(self, query: str, max_price: float = None, min_price: float = None, limit: int = 10) -> List[Dict]:
        """Recherche avec filtrage de prix"""
        logger.info(f"üîç Recherche avec filtrage de prix: '{query}' (max: {max_price}, min: {min_price})")
        
        # 1. Recherche s√©mantique pour comprendre l'intention
        semantic_results = self.semantic_search(query, min_score=0.6)
        logger.info(f"üß† R√©sultats s√©mantiques: {len(semantic_results)}")
        
        # 2. Recherche textuelle pour les correspondances exactes
        try:
            all_announcements = []
            offset = 0
            page_limit = 25
            
            while True:
                response = self.db.list_documents(
                    database_id=DATABASE_ID, 
                    collection_id=COLLECTION_ID, 
                    queries=[
                        Query.limit(page_limit),
                        Query.offset(offset)
                    ]
                )
                announcements = response['documents']
                
                if len(announcements) == 0:
                    break
                
                all_announcements.extend(announcements)
                offset += page_limit
                
                if len(announcements) < page_limit:
                    break
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration annonces: {e}")
            return []
        
        # Recherche textuelle
        text_results = self.text_search(query, all_announcements)
        logger.info(f"üìù R√©sultats textuels: {len(text_results)}")
        
        # 3. Combiner et filtrer par prix
        combined_results = []
        seen_ids = set()
        
        # Ajouter r√©sultats textuels
        for result in text_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # Ajouter r√©sultats s√©mantiques
        for result in semantic_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # 4. Filtrer par prix
        filtered_results = []
        for result in combined_results:
            price = result.get('price', 0)
            
            # V√©rifier les contraintes de prix
            if max_price is not None and price > max_price:
                continue
            if min_price is not None and price < min_price:
                continue
                
            filtered_results.append(result)
        
        # 5. Trier par score et limiter
        filtered_results.sort(key=lambda x: x['score'], reverse=True)
        filtered_results = filtered_results[:limit]
        
        logger.info(f"‚úÖ Recherche avec filtrage: {len(filtered_results)} r√©sultats (sur {len(combined_results)} total)")
        
        return filtered_results
    
    def _get_announcement_details(self, announcement_id: str) -> Dict[str, Any]:
        """R√©cup√®re les d√©tails complets d'une annonce depuis Appwrite"""
        if not self.db or not announcement_id:
            return None
        
        try:
            response = self.db.get_document(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_ID,
                document_id=announcement_id
            )
            return response
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des d√©tails pour {announcement_id}: {e}")
            return None
    
    def hybrid_search(self, query: str, limit: int = 5) -> Dict[str, Any]:
        """Recherche hybride combinant textuelle et s√©mantique"""
        logger.info(f"üîç D√©but de la recherche hybride pour: '{query}' (limit: {limit})")
        
        # R√©cup√©rer toutes les annonces pour la recherche textuelle
        try:
            logger.info("üì• R√©cup√©ration des annonces depuis Appwrite...")
            # R√©cup√©rer toutes les annonces avec pagination
            all_announcements = []
            offset = 0
            page_limit = 25
            page_count = 0
            
            while True:
                page_count += 1
                logger.info(f"üìÑ R√©cup√©ration page {page_count} (offset: {offset})")
                
                response = self.db.list_documents(
                    database_id=DATABASE_ID, 
                    collection_id=COLLECTION_ID, 
                    queries=[
                        Query.limit(page_limit),
                        Query.offset(offset)
                    ]
                )
                announcements = response['documents']
                
                if len(announcements) == 0:
                    logger.info("üèÅ Fin de pagination (aucune annonce)")
                    break
                
                all_announcements.extend(announcements)
                offset += page_limit
                
                # Si on a moins d'annonces que la limite, c'est la derni√®re page
                if len(announcements) < page_limit:
                    logger.info("üèÅ Derni√®re page atteinte")
                    break
                
            logger.info(f"üìä Total d'annonces r√©cup√©r√©es: {len(all_announcements)}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des annonces: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"error": "Impossible de r√©cup√©rer les annonces"}
        
        # Recherche textuelle
        logger.info("üîç Ex√©cution de la recherche textuelle...")
        text_results = self.text_search(query, all_announcements)
        logger.info(f"üìù R√©sultats textuels trouv√©s: {len(text_results)}")
        
        # Recherche s√©mantique avec seuil strict
        logger.info("üß† Ex√©cution de la recherche s√©mantique...")
        semantic_results = self.semantic_search(query, min_score=0.7)
        logger.info(f"üß† R√©sultats s√©mantiques trouv√©s: {len(semantic_results)}")
        
        # Combiner et d√©dupliquer les r√©sultats
        logger.info("üîó Combinaison des r√©sultats...")
        combined_results = []
        seen_ids = set()
        
        # Priorit√© aux r√©sultats textuels (correspondance exacte)
        for result in text_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # Ajouter les r√©sultats s√©mantiques
        for result in semantic_results:
            if result['id'] not in seen_ids:
                combined_results.append(result)
                seen_ids.add(result['id'])
        
        # Trier par score et limiter
        combined_results.sort(key=lambda x: x['score'], reverse=True)
        combined_results = combined_results[:limit]
        
        logger.info(f"‚úÖ Recherche termin√©e: {len(combined_results)} r√©sultats finaux")
        
        return {
            "query": query,
            "total_results": len(combined_results),
            "text_results": len(text_results),
            "semantic_results": len(semantic_results),
            "results": combined_results
        }

def interactive_search():
    """Recherche interactive"""
    
    print("üîç Recherche hybride interactive")
    print("=" * 40)
    
    # Demander la cl√© API OpenAI
    print("üîë Veuillez entrer votre cl√© API OpenAI:")
    print("   (ou appuyez sur Entr√©e pour utiliser la cl√© par d√©faut)")
    
    api_key = input("Cl√© API OpenAI: ").strip()
    if not api_key:
        print("‚ùå Veuillez fournir une cl√© API OpenAI valide")
        return
    
    # Initialiser l'API
    api = HybridSearchAPI(api_key)
    
    if not api.vectorstore:
        print("‚ùå Impossible d'initialiser l'API")
        return
    
    print("\nüéØ Recherche interactive")
    print("-" * 30)
    
    while True:
        query = input("\nüîé Entrez votre requ√™te de recherche (ou 'quit' pour quitter): ").strip()
        if query.lower() == 'quit':
            break
        
        if query:
            results = api.hybrid_search(query, limit=5)
            
            if "error" in results:
                print(f"  ‚ùå Erreur: {results['error']}")
            else:
                print(f"  üìà R√©sultats trouv√©s: {results['total_results']}")
                print(f"    - Correspondances textuelles: {results['text_results']}")
                print(f"    - Correspondances s√©mantiques: {results['semantic_results']}")
                
                if results['total_results'] == 0:
                    print("  ‚ö†Ô∏è Aucun r√©sultat trouv√©.")
                else:
                    for i, result in enumerate(results['results'], 1):
                        print(f"    R√©sultat {i}:")
                        print(f"      Titre: {result['title']}")
                        print(f"      Type: {result['match_type']}")
                        print(f"      Score: {result['score']:.4f}")
                        print(f"      Prix: {result['price']} ‚Ç¨")
                        print(f"      Localisation: {result['location']}")
                        print()
    
    print("\n‚úÖ Recherche termin√©e!")

if __name__ == "__main__":
    interactive_search() 